{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"Fig/ENSIMAG.png\" width=\"30%\" height=\"30%\"></center>\n",
    "<center><h3>Ensimag 2A</h3></center>\n",
    "<hr>\n",
    "<center><h1>Optimisation Numérique</h1></center>\n",
    "<center><h2>TP: Algorithmes de descente de gradient et (quasi-) Newton</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Introduction\n",
    "\n",
    "L'objectif de ce TP est de se familiariser avec les algorithmes d'optimisation numérique en codant deux algorithmes de base et en observant leur comportement sur deux fonctions simples.\n",
    "\n",
    "---\n",
    "\n",
    "L’ensemble d’un programme d'optimisation se présente en deux parties :\n",
    "* *un simulateur :* il est chargé de calculer la fonction (ainsi que le gradient et éventuellement la Hessienne) en chaque point décidé par l’algorithme. Souvent, la fonction à minimiser n’est connue que via ce simulateur.\n",
    "* *l’algorithme proprement dit :* il comporte dans notre cas deux boîtes principales correspondant au calcul de la direction, et au calcul du pas.\n",
    "\n",
    "--- \n",
    "En pratique, nous allons séparer *simulateurs* et *partie algorithmique* de la manière suivante :\n",
    "* `TP.ipynb`, le présent fichier, contiendra l'énoncé, les algorithmes développés, et sera l'environnement d'éxécution et d'affichage\n",
    "* `src/sim_f1.py` et `src/sim_f2.py` contiendront les simulateurs des fonctions étudiées, à compléter.\n",
    "* Dans `src` les squelettes des trois algorithmes étudiés sont fournis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import start\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example d'import d'un simulateur :** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.sim_f1 import SimF1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 1.  Simulateurs\n",
    "\n",
    "> Ecrire dans les fichiers respectifs `src/sim_f1.py` et `src/sim_f2.py`  un simulateur pour les fonctions suivantes :\n",
    "> $$\\begin{array}{rl} f_1(x) =& \\sum_{k=1}^n k x_k^2 ~~~ \\text{ pour }  x\\in\\mathbb{R}^n\\\\ f_2(x) =& (1-x_1)^2 + 100(x_2-x_1^2)^2 ~~~ \\text{ pour }  x=(x_1,x_2)\\in\\mathbb{R}^2\\end{array}$$ \n",
    "\n",
    "Calculer (sur papier) les minima globaux de ces deux fonctions.\n",
    "\n",
    "Le format du simulateur doit etre\n",
    "\n",
    "```python\n",
    "class MonSimulateur\n",
    "    def sim(self, x):\n",
    "        # Calcul de la valeur de la fonction et de son gradient en x\n",
    "        f = ...(x)\n",
    "        g = ...(x)\n",
    "        return f, g  # Ou (f, g, h) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "avec `x` la valeur de la variable pour laquelle il faut évaluer $f$ et son gradient, `f` qui vaut $f(x)$ et `g` qui vaut $\\nabla f(x)$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 2.  Lignes de niveaux et plot 3D\n",
    "\n",
    "> Visualiser la géométrie des lignes de niveaux ainsi que le tracé 3D de la fonction pour $f_1$ avec $n=2$ et $f_2$ à l'aide des méthodes `level_plot` et `custom_3dplot`. Les paramètres de ces tracés peuvent être changés dans les fichiers des simulateurs respectifs. Discuter la convexité des deux fonctions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fonction $f_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "f1 = SimF1(2, 300)\n",
    "\n",
    "f1.custom_3dplot()\n",
    "f1.level_plot()\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fonction $f_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from src.sim_f2 import SimF2\n",
    "\n",
    "f2 = SimF2(200)\n",
    "f2.custom_3dplot()\n",
    "f2.level_plot()\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 3. Algorithme de gradient à pas constant\n",
    "\n",
    "Commençons par un algorithme de descente simple : la méthode du gradient à pas constant. À chaque itération, on fixe deux choses : la direction de descente $d = -\\nabla f(x)$ et le pas $\\gamma>0$, la méthode s'écrit alors \n",
    "$$ x_{k+1} = x_k - \\gamma \\nabla f(x_k). $$\n",
    "\n",
    "> Créer une fonction qui prendra en entrée \n",
    "> * `sim` le nom du simulateur\n",
    "> * `x0` le point initial\n",
    "> * `gamma` le pas (constant)\n",
    "> * `PREC` la précision attendue (la norme du gradient au point final souhaitée)\n",
    "> * `ITE_MAX` le nombre maximal d'itérations à faire\n",
    "\n",
    "> Retourner le point final, ainsi que le tableau $k\\times n$ des iterées où $k$ est le nombre d'itérations effectuées.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from src.gd import GradDescent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appliquer l'algorithme précédent pour minimiser $f_1$ en dimension 2 à partir de $x_0 = (-1,-1)$. \n",
    "> Tester plusieurs pas de descente : $\\gamma = 0.1$, puis $\\gamma = 0.5$, et $\\gamma=1$. \n",
    "\n",
    "> Tester ensuite en dimension supérieure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotLib import *\n",
    "%matplotlib inline\n",
    "\n",
    "#### Parameter we give at our algorithm\n",
    "PREC    = 0.001                      # Sought precision\n",
    "ITE_MAX = 10                         # Max number of iterations\n",
    "x0      = np.array( [-1.0 , -1.0] )  # Initial point\n",
    "gamma   =  0.1                       # Stepsize\n",
    "\n",
    "gd = GradDescent(ITE_MAX, f1, x0, gamma)\n",
    "finished, x_tab = gd.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Dans le cas $n=2$, tracer les itérées sur les lignes de niveau de la fonction par la fonction `level_points_plot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "f1.level_points_plot(x_tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De même pour minimiser $f_2$ : \n",
    "\n",
    "> Expérimenter différents paramètres à partir de $x_0= (-1,1.2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "#### Parameter we give at our algorithm\n",
    "PREC    = 0.0001                     # Sought precision\n",
    "ITE_MAX = 10000                      # Max number of iterations\n",
    "x0      = np.array( [ -1 , 1.2] )    # Initial point\n",
    "gamma   = 0.0005                      # Stepsize\n",
    "\n",
    "\n",
    "gd = GradDescent(ITE_MAX, f2, x0, gamma)\n",
    "finished, x_tab = gd.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tracer les itérées sur les lignes de niveau de la fonction par la fonction `level_points_plot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotLib import *\n",
    "%matplotlib inline\n",
    "\n",
    "f2.level_points_plot(x_tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 4. Recherche linéaire\n",
    "\n",
    "La question précendente montre la difficulté de choisir un bon pas de descente. \n",
    "\n",
    "Une manière de choisir un pas $\\gamma$ satisfaisant est de tester différents pas en appellant plusieurs fois le simulateur. Cette procédure s'appelle recherche linéaire ; la recherche linéaire de Wolfe est implémentée dans la fonction `Wolfe_line_search`  fournie dans la bibliothèque `custom_optim` ci-jointe.\n",
    "\n",
    "**`function`** *Wolfe_line_search(sim,xk,pk)* \n",
    "* Input:\n",
    "    * `sim` a function simulator\n",
    "    * `xk` the current point\n",
    "    * `pk` the current descent direction\n",
    "* Output:\n",
    "    * `alpha` an admissible stepsize satisfying Wolfe conditions (`double`) or `None` if the method did not converge \n",
    "\n",
    "**Méthode de Wolfe.** Soit un point $x$, une direction de descente $d$, et $q(t)=f(x+\\gamma d)$. La recherche linéaire de Wolfe consiste à decider que\n",
    "* $\\gamma$ est satisfaisant si $q(t)\\leq q(0)+m_1 \\gamma q'(0)$ et $q'(t)\\geq m_2 q'(0)$;\n",
    "* $\\gamma$ est trop grand si $q(t) > q(0)+m_1 \\gamma q'(0)$;\n",
    "* $\\gamma$ est trop petit si $q(t)\\leq q(0)+m_1 \\gamma q'(0)$ et $q'(t)<m_2 q'(0)$;\n",
    "\n",
    "pour deux constantes $0<m_1<m_2<1$, par exemple : $m1 = 0.1, m2 = 0.9$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Ecrire un algorithme de gradient avec cette recherche linéaire. Le tester sur $f_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.wolfe import WolfeLineSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Parameter we give at our algorithm\n",
    "PREC    = 1e-6                     # Sought precision\n",
    "ITE_MAX = 10000                      # Max number of iterations\n",
    "x0      = np.array( [ -1 , 1.2] )    # Initial point\n",
    "\n",
    "wolfe = WolfeLineSearch(ITE_MAX, f2, x0, prec=PREC)\n",
    "finished, x_tab = wolfe.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "f2.level_points_plot(x_tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 5. Quasi Newton\n",
    "\n",
    "Nous avons maintenant une méthode pour choisir un bon pas, nous allons à présent calculer une bonne direction.\n",
    "\n",
    "Pour une fonction $f$ differentiable, les méthodes de Quasi-Newton construisent successivement une approximation $W_k$ de l'inverse de la Hessienne de la fonction et utilisent la direction $-W_k\\nabla f(x_k)$.\n",
    "\n",
    "**BFGS.** (Broyden-Fletcher-Goldfarb-Shanno, 1970) L'algorithme BFGS consiste à réaliser l'itération\n",
    "$$ x_{k+1}=x_k - \\gamma_k W_k \\nabla f(x_k)$$\n",
    "où $\\gamma_k$ est donné par la recherche linéaire de Wolfe et la matrice symmétrique définie positive $W_k$ est\n",
    " calculée par la formule de récurrence\n",
    "$$ W_{k+1}=W_k - \\frac{s_k y_k^T W_k+W_k y_k s_k^T}{y_k^T s_k} +\\left[1+\\frac{y_k^T W_k y_k}{y_k^T s_k}\\right]\\frac{s_k s_k^T}{y_k^T s_k} $$\n",
    "avec $s_k=x_{k+1}-x_{k}$ et $y_k=\\nabla f(x_{k+1}) - \\nabla f(x_{k})$.\n",
    "\n",
    "Le schéma général d'une méthode de quasi-Newton est alors :\n",
    "* avec l'itéré initial $x_0$, se donner une matrice initiale $W_0$ symétrique définie positive;\n",
    "* connaissant le gradient $\\nabla f(x_k)$, calculer la direction $d_k=-W_k \\nabla f(x_k)$;\n",
    "* calculer le pas $\\gamma_k$ par recherche linéaire de Wolfe;\n",
    "* connaissant le nouvel itéré $x_{k+1}$, appeler le simulateur et calculer la nouvelle matrice $W_{k+1}$.\n",
    "\n",
    "> Implémenter la méthode BFGS et la tester sur la fonction $f_2$.\n",
    "\n",
    "*Indication: utiliser la fonction `np.outer(a,b)` to compute $ab^T$.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from src.bfgs import BFGSDescent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Parameter we give at our algorithm\n",
    "PREC    = 1e-6                    # Sought precision\n",
    "ITE_MAX = 10000                      # Max number of iterations\n",
    "x0      = np.array( [ -1 , 1.2] )    # Initial point\n",
    "\n",
    "bfgs = BFGSDescent(ITE_MAX, f2, x0, prec=PREC)\n",
    "finished, x_tab = bfgs.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tracer les itérées sur les lignes de niveau de la fonction par la fonction `level_points_plot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2.level_points_plot(x_tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Newton\n",
    "\n",
    "> Créer dans les fichiers `sim_f1.ipynb` et `sim_f2.ipynb` un nouveau simulateur retournant également la Hessienne des fonctions respectives.\n",
    "> Implémenter la méthode de Newton $$ x_{k+1} = x_k - [\\nabla^2f(x_k)]^{-1} \\nabla f(x_k)$$ et comparer avec Quasi-Newton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(gd.vals, label=\"gd\")\n",
    "plt.plot(wolfe.vals, label=\"wolfe\")\n",
    "plt.plot(bfgs.vals, label=\"bfgs\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
