{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"Fig/Ensimag.png\" width=\"30%\" height=\"30%\"></center>\n",
    "<center><h3>Ensimag 2A</h3></center>\n",
    "<hr>\n",
    "<center><h1>Optimisation Num√©rique</h1></center>\n",
    "<center><h2>TP3: Proximal Algorithms (2x1.5h)</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structure of an optimization program\n",
    "\n",
    "An optimization program can be practically divided into three parts:\n",
    "* the *run* environment, in which you test, run your program, and display results.\n",
    "* the *problem* part, which contains the function oracles, problem constraints, etc.\n",
    "* the *algorithmic* part, where the algorithms are coded.\n",
    "\n",
    "The main interest of such division is that these parts are interchangeable, meaning that, for instance, the algorithms of the third part can be used of a variety of problems. That is why such a decomposition is widely used.\n",
    "\n",
    "In the present lab, you will use this division:\n",
    "* `TP3_Proximal_algorithms.ipynb` will be the *run* environment\n",
    "* `logreg.py` will be the considered *logistic regression problem* for this lab\n",
    "* `prox.py` will contain the proximal *algorithms* studied in this lab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Composite minimization for machine learning.\n",
    "\n",
    "In this lab, we will investigate optimization algorithms over composite functions composed of a smooth and a non-smooth part using the proximal gradient algorithm over a practical problem of machine learning: binary classification using logistic regression.</br>\n",
    "\n",
    "> + Read the file [`logistic_regression_2.ipynb`](logistic_regression_2.ipynb) containing the problem explanation and simulators.\n",
    ">\n",
    "> + Implement the proximal operation linked to $\\ell_1$ norm in the regularization ($g$ function).\n",
    ">\n",
    "> + Implement the proximal gradient algorithm in the file [`src/prox.py`](src/prox.py) and test your algorithm below. Remember to tune the stepsize in the variable `step` bellow, according to what you learned in class about L-smooth functions and GD algorithms. Notice that the $f$ part of the loss is $L$-smooth (find $L$ in the code).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick recap of the proximal gradient function\n",
    "\n",
    "For minimizing a function $F:\\mathbb{R}^n \\to \\mathbb{R}$ equal to $f+g$ where $f$ is differentiable and the $\\mathbf{prox}$ of $g$ is known, given:\n",
    "* the function to minimize `F`\n",
    "* a 1st order oracle for $f$ `f_grad` \n",
    "* a proximity operator for $g$ `g_prox` \n",
    "* an initialization point `x0`\n",
    "* the sought precision `PREC` \n",
    "* a maximal number of iterations `ITE_MAX` \n",
    "* a display boolean variable `PRINT` \n",
    "\n",
    "these algorithms perform iterations of the form\n",
    "$$ x_{k+1} = \\mathbf{prox}_{\\gamma g}\\left( x_k - \\gamma \\nabla f(x_k) \\right) := \\text{arg}\\min_{z}\\left\\lbrace \\gamma g(z) + \\frac{1}{2}\\|z - x_k + \\gamma \\nabla f(x_k)\\|_2^2\\right\\rbrace $$\n",
    "where $\\gamma$ is a stepsize to choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.prox import Proximal\n",
    "from src.logistic import SimulatorStudentsDataset\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "lreg = SimulatorStudentsDataset(\"Logistic regression\")\n",
    "#### Parameter we give at our algorithm\n",
    "PREC    = 1e-5                     # Sought precision\n",
    "ITE_MAX = 1000                      # Max number of iterations\n",
    "x0      = np.zeros(lreg.n)              # Initial point\n",
    "step    = ...                      # FILL HERE\n",
    "\n",
    "##### gradient algorithm\n",
    "proxalg = Proximal(ITE_MAX, lreg, x0, step, prec=PREC)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Investigate the decrease of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "rcode, x_tab = proxalg.run()\n",
    "lreg.plot_loss(x_tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Plot, with the following command, the support of the vector $x_k$ (i.e. one point for every non-null coordinate of $x_k$) versus the iterations. \n",
    "\n",
    "> What do yo notice? Was it expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lreg.plot_support(x_tab, period=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Regularization path.\n",
    "\n",
    "\n",
    "We saw above that the algorithm *selected* some coordinates as the other get to zero. Considering our machine learning task (see `logistic_regression_2.ipynb`), this translates into the algorithm selecting a subset of the features that will be used for the prediction step.  \n",
    "\n",
    "> Change the parameter $\\lambda_1$ of the problem (`lreg.lam1`) in the code above and investigate how it influences the number of selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "#### Parameter we give at our algorithm (see algoGradient.ipynb)\n",
    "PREC    = 1e-5                     # Sought precision\n",
    "ITE_MAX = 500                      # Max number of iterations\n",
    "x0      = np.zeros(pb.n)              # Initial point\n",
    "step = ...  # FILL HERE\n",
    "\n",
    "# FILL THERE #######\n",
    "reg_l1 = ...\n",
    "\n",
    "lreg = SimulatorStudentsDataset(f\"Logistic regression $\\\\lambda_1={reg_l1}$\")\n",
    "proxalg = Proximal(ITE_MAX, lreg, x0, step, prec=PREC)\n",
    "\n",
    "retcode, x_tab = proxalg.run()\n",
    "lreg.plot_support(x_tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to quantify the influence of this feature selection, let us consider the *regularization path* that is the support of the final points obtained by our minimization method versus the value of $\\lambda_1$.\n",
    "\n",
    "> For $\\lambda_1 = 2^{-12},2^{-11}, .. , 2^{1}$, run the proximal gradient algorithm on the obtained problem and store the support of the final point, the prediction performance on the *training set* (`lreg.prediction_train(...)`) and on the *testing set* (`lreg.prediction_test(...)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILL THERE #######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Plot the *regularization path* and look at the feature signification (file `student.txt` or `logistic_regression_2.ipynb`) to see which are the most important features of the dataset.\n",
    "\n",
    "> (Bonus: you can do some text manipulation to put the labels on the plot as well)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILL THERE #######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Plot the *training* and *testing* accuracies versus the value of $\\lambda_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_lam = np.arange(-11, 2)\n",
    "# FILL HERE ####\n",
    "# ##############\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Explore the proximal algorithm or propose ideas (cite your sources if you use pieces of litterature) to change it or compare it to something else. Send the results to your favorite TA by zipping/tarballing/... your work and either:\n",
    "> * sending it directly via an email.\n",
    "> * sending and email with an invitation to a PRIVATE repository (github, gitlab, bitbucket, etc) containing your work.\n",
    ">\n",
    "> ### Guidelines:\n",
    "> Write your own code, do not try to throw LLM nonsense to your TA.\n",
    "> \n",
    "> Be original.\n",
    "> \n",
    "> Write every idea you have in the notebook, as verbosely as possible.\n",
    ">\n",
    "> Write clean code. E.g. go see python's pep8. (Notice that I did not follow it myself for the template code, as some variables are linked to mathematical notations which do not respect pep8)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
